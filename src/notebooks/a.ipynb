{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import random, seed\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import functions as f\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#plt.style.available\n",
    "import seaborn as sns\n",
    "import load_data as ld\n",
    "import classes as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.88114956 0.82005649 0.73665888]\n"
     ]
    }
   ],
   "source": [
    "# Load simple data\n",
    "X, y, x, a_true = ld.load_simple_data(100, 0.1)\n",
    "# Initial beta\n",
    "beta_init = np.random.randn(X.shape[1])\n",
    "print(X.shape[1])\n",
    "print(a_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.22541231  1.12483019 -1.18774191]\n",
      "[0.88378481 0.82957288 0.72425269]\n",
      "[0.87919261 0.82554568 0.72457299]\n"
     ]
    }
   ],
   "source": [
    "betaOLS = f.beta_OLS(X, y)\n",
    "betaRidge = f.beta_Ridge(X, y, 0.5)\n",
    "print(beta_init)\n",
    "print(betaOLS)\n",
    "print(betaRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26089698 0.95388038 0.89253865]\n",
      "0.2731105568226006\n",
      "[0.27542128 0.89161104 0.86313741]\n",
      "0.2641180693645645\n"
     ]
    }
   ],
   "source": [
    "#Task 1) plain gradient descent with and without ridge\n",
    "#OLS)\n",
    "gd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "#Ridge)\n",
    "rgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge')\n",
    "optimized_rgd_beta = rgd.optimize()\n",
    "print(optimized_rgd_beta)\n",
    "yrgd = optimized_rgd_beta[0] + optimized_rgd_beta[1]*x + optimized_rgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8888034  0.82885954 0.72834302]\n",
      "0.01127364104208621\n",
      "[0.80079571 0.75458433 0.72936325]\n",
      "0.023232293923448836\n"
     ]
    }
   ],
   "source": [
    "#task 2) momentum gradient descent with and without ridge and comparison to plain gradient descent\n",
    "#OLS)\n",
    "mgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "print(optimized_mgd_beta)\n",
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ymgd))\n",
    "#Ridge)\n",
    "rmgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge', momentum = 0.9)\n",
    "optimized_rmgd_beta = rmgd.optimize()\n",
    "print(optimized_rmgd_beta)\n",
    "yrmgd = optimized_rmgd_beta[0] + optimized_rmgd_beta[1]*x + optimized_rmgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrmgd))\n",
    "\n",
    "#comparing with the above loss values, we clearly see faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 3) repeat for stochastic gradient descent and dicuss results with respect to batch size, number of epochs etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88390167 0.82956332 0.72419535]\n",
      "[0.88378481 0.82957288 0.72425269]\n",
      "[0.88378481 0.82957288 0.72425269]\n",
      "[0.88378481 0.82957288 0.72425269]\n",
      "[0.89224839 0.82003571 0.752102  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='gd', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "\n",
    "agd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_agd_beta = agd.optimize()\n",
    "\n",
    "magd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_magd_beta = magd.optimize()\n",
    "\n",
    "sgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adam', gradient_method='analytical')\n",
    "optimized_sgd_beta = sgd.optimize()\n",
    "\n",
    "print(optimized_mgd_beta)\n",
    "print(optimized_agd_beta)\n",
    "print(optimized_magd_beta)\n",
    "print(optimized_sgd_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011165065284857561\n",
      "0.01116505656129741\n",
      "0.011165056561297406\n",
      "0.01116505656129741\n",
      "0.0137657053119865\n"
     ]
    }
   ],
   "source": [
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "yagd = optimized_agd_beta[0] + optimized_agd_beta[1]*x + optimized_agd_beta[2]*x**2\n",
    "ymagd = optimized_magd_beta[0] + optimized_magd_beta[1]*x + optimized_magd_beta[2]*x**2\n",
    "\n",
    "\n",
    "ysgd = optimized_sgd_beta[0] + optimized_sgd_beta[1]*x + optimized_sgd_beta[2]*x**2\n",
    "\n",
    "print(mean_squared_error(y, ymgd))\n",
    "print(mean_squared_error(y, yagd))\n",
    "#adagrad converges slower. For higher epochs it converges similarly to the others\n",
    "print(mean_squared_error(y, ymagd))\n",
    "print(mean_squared_error(y, ysgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "sort_inds = np.argsort(x)\n",
    "plt.plot(x[sort_inds], y[sort_inds], label='Datapoints')\n",
    "plt.plot(x[sort_inds], ygd[sort_inds], label='Gradient Descent')\n",
    "plt.plot(x[sort_inds], ysgd[sort_inds], label='Stochastic Gradient Descent')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
