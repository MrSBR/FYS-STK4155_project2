{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import random, seed\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import functions as f\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#plt.style.available\n",
    "import seaborn as sns\n",
    "import load_data as ld\n",
    "import classes as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[0.88114956 0.82005649 0.73665888]\n"
     ]
    }
   ],
   "source": [
    "# Load simple data\n",
    "X, y, x, a_true = ld.load_simple_data(100, 0.1)\n",
    "# Initial beta\n",
    "beta_init = np.random.randn(X.shape[1])\n",
    "print(X.shape[1])\n",
    "print(a_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.22541231  1.12483019 -1.18774191]\n",
      "[0.88378481 0.82957288 0.72425269]\n",
      "[0.87919261 0.82554568 0.72457299]\n"
     ]
    }
   ],
   "source": [
    "betaOLS = f.beta_OLS(X, y)\n",
    "betaRidge = f.beta_Ridge(X, y, 0.5)\n",
    "print(beta_init)\n",
    "print(betaOLS)\n",
    "print(betaRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26089698 0.95388038 0.89253865]\n",
      "0.2731105568226006\n",
      "[0.27542128 0.89161104 0.86313741]\n",
      "0.2641180693645645\n"
     ]
    }
   ],
   "source": [
    "#Task 1) plain gradient descent with and without ridge\n",
    "#OLS)\n",
    "gd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "#Ridge)\n",
    "rgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge')\n",
    "optimized_rgd_beta = rgd.optimize()\n",
    "print(optimized_rgd_beta)\n",
    "yrgd = optimized_rgd_beta[0] + optimized_rgd_beta[1]*x + optimized_rgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8888034  0.82885954 0.72834302]\n",
      "0.01127364104208621\n",
      "[0.80079571 0.75458433 0.72936325]\n",
      "0.023232293923448836\n"
     ]
    }
   ],
   "source": [
    "#task 2) momentum gradient descent with and without ridge and comparison to plain gradient descent\n",
    "#OLS)\n",
    "mgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "print(optimized_mgd_beta)\n",
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ymgd))\n",
    "#Ridge)\n",
    "rmgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100, optimizer='gd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge', momentum = 0.9)\n",
    "optimized_rmgd_beta = rmgd.optimize()\n",
    "print(optimized_rmgd_beta)\n",
    "yrmgd = optimized_rmgd_beta[0] + optimized_rmgd_beta[1]*x + optimized_rmgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrmgd))\n",
    "\n",
    "#comparing with the above loss values, we clearly see faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87438311 0.83036964 0.72878924]\n",
      "0.01122120238993803\n",
      "[0.80506899 0.75783239 0.72770954]\n",
      "0.022293161955824176\n",
      "[0.88569882 0.83756458 0.72124937]\n",
      "0.011245525459773037\n",
      "[0.80079253 0.76183365 0.73346842]\n",
      "0.02166172444647367\n",
      "[-0.56968465  1.14375893 -0.20019443]\n",
      "[0.32812029 0.89491222 0.86760618]\n",
      "[0.89282688 0.8299781  0.72529652]\n",
      "[-0.62871363  1.06069089 -0.34563259]\n",
      "[0.27766815 0.93363299 0.86853585]\n",
      "[0.88184548 0.83200338 0.7253477 ]\n",
      "[-0.6007769   1.10099537 -0.13561428]\n",
      "[0.24058679 0.92417854 0.91830655]\n",
      "[0.87850238 0.83116379 0.72433132]\n",
      "Batch size: 1, Epochs: 20, Loss: 7.030281394248226\n",
      "Batch size: 1, Epochs: 100, Loss: 0.21599331125174054\n",
      "Batch size: 1, Epochs: 1000, Loss: 0.011268597852969131\n",
      "Batch size: 5, Epochs: 20, Loss: 8.441998483913887\n",
      "Batch size: 5, Epochs: 100, Loss: 0.26445325161313\n",
      "Batch size: 5, Epochs: 1000, Loss: 0.011173072021049488\n",
      "Batch size: 20, Epochs: 20, Loss: 6.672553106064881\n",
      "Batch size: 20, Epochs: 100, Loss: 0.2776301607898716\n",
      "Batch size: 20, Epochs: 1000, Loss: 0.011194147979597032\n"
     ]
    }
   ],
   "source": [
    "#task 3) repeat for stochastic gradient descent and dicuss results with respect to batch size, number of epochs etc\n",
    "#without momentum\n",
    "#OLS)\n",
    "gd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='sgd', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "#Ridge)\n",
    "rgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='sgd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge')\n",
    "optimized_rgd_beta = rgd.optimize()\n",
    "print(optimized_rgd_beta)\n",
    "yrgd = optimized_rgd_beta[0] + optimized_rgd_beta[1]*x + optimized_rgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrgd))\n",
    "#with momentum\n",
    "#OLS)\n",
    "mgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='sgd', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "print(optimized_mgd_beta)\n",
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ymgd))\n",
    "#Ridge)\n",
    "rmgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='sgd', gradient_method='analytical', lambda_param=0.1, cost_function='ridge', momentum = 0.9)\n",
    "optimized_rmgd_beta = rmgd.optimize()\n",
    "print(optimized_rmgd_beta)\n",
    "yrmgd = optimized_rmgd_beta[0] + optimized_rmgd_beta[1]*x + optimized_rmgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, yrmgd))\n",
    "\n",
    "#this clearly converges faster than standard gradient descent (here we have batch sizes 1/10th the size, and 10x the epochs,\n",
    "#so the same amount of computations.\n",
    "\n",
    "#testing various batch sizes and number of epochs using OLS without momentum\n",
    "batch_sizes = [1, 5, 20]\n",
    "epochsls = [20, 100, 1000]\n",
    "cost_history = []\n",
    "for batch_size in batch_sizes:\n",
    "    for epochs in epochsls:\n",
    "        gd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=epochs, optimizer='sgd', gradient_method='analytical', lambda_param=0.1, batch_size=batch_size)\n",
    "        optimized_gd_beta = gd.optimize()\n",
    "        print(optimized_gd_beta)\n",
    "        ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "        cost_history.append((batch_size, epochs, mean_squared_error(y, ygd)))\n",
    "\n",
    "for t in cost_history:\n",
    "    print(f\"Batch size: {t[0]}, Epochs: {t[1]}, Loss: {t[2]}\")\n",
    "#for this simple of a function, it seems that every single datapoint gives a gradient representative of the whole dataset\n",
    "#that is shown by the batch_size increasing doesn't seem to matter much for the loss, while increasing epochs dramatically\n",
    "#decreases the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.64350203  0.84227052 -0.60711252]\n",
      "10.852616470567828\n",
      "[0.86515627 0.83041595 0.73453607]\n",
      "0.011400314778876468\n",
      "[0.85634876 0.83082015 0.73923755]\n",
      "0.01167183728398292\n",
      "[-0.65442168  1.0723386  -0.70922662]\n",
      "11.994761205429096\n",
      "[0.95040871 0.82886354 0.67653092]\n",
      "0.0150786043134716\n"
     ]
    }
   ],
   "source": [
    "#task 4, using adagrad on all previous methods (don't test for ridge since it would simply bloat the code)\n",
    "#OLS, no momentum, standard gradient descent\n",
    "gd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "    #repeat with 100x higher epochs to show that adagrad slows down convergence\n",
    "gd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=100000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "#OLS, momentum, standard gradient descent\n",
    "mgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "print(optimized_mgd_beta)\n",
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ymgd))\n",
    "#OLS, momentum, stochastic standard gradient descent\n",
    "gd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_gd_beta = gd.optimize()\n",
    "print(optimized_gd_beta)\n",
    "ygd = optimized_gd_beta[0] + optimized_gd_beta[1]*x + optimized_gd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ygd))\n",
    "#OLS, momentum, stochastic standard gradient descent\n",
    "mgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "print(optimized_mgd_beta)\n",
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "print(mean_squared_error(y, ymgd))\n",
    "\n",
    "\n",
    "#so we see that adagrad heavily slows down convergence. It can be balanced out by using momentum,\n",
    "#as we see the implementations with momentum converge much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mgd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='gd', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_mgd_beta = mgd.optimize()\n",
    "\n",
    "agd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1)\n",
    "optimized_agd_beta = agd.optimize()\n",
    "\n",
    "magd = cl.GradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adagrad', gradient_method='analytical', lambda_param=0.1, momentum = 0.9)\n",
    "optimized_magd_beta = magd.optimize()\n",
    "\n",
    "sgd = cl.StochasticGradientDescent(X, y, beta_init, learning_rate=0.01, epochs=1000, optimizer='adam', gradient_method='analytical')\n",
    "optimized_sgd_beta = sgd.optimize()\n",
    "\n",
    "print(optimized_mgd_beta)\n",
    "print(optimized_agd_beta)\n",
    "print(optimized_magd_beta)\n",
    "print(optimized_sgd_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymgd = optimized_mgd_beta[0] + optimized_mgd_beta[1]*x + optimized_mgd_beta[2]*x**2\n",
    "yagd = optimized_agd_beta[0] + optimized_agd_beta[1]*x + optimized_agd_beta[2]*x**2\n",
    "ymagd = optimized_magd_beta[0] + optimized_magd_beta[1]*x + optimized_magd_beta[2]*x**2\n",
    "\n",
    "\n",
    "ysgd = optimized_sgd_beta[0] + optimized_sgd_beta[1]*x + optimized_sgd_beta[2]*x**2\n",
    "\n",
    "print(mean_squared_error(y, ymgd))\n",
    "print(mean_squared_error(y, yagd))\n",
    "#adagrad converges slower. For higher epochs it converges similarly to the others\n",
    "print(mean_squared_error(y, ymagd))\n",
    "print(mean_squared_error(y, ysgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "sort_inds = np.argsort(x)\n",
    "plt.plot(x[sort_inds], y[sort_inds], label='Datapoints')\n",
    "plt.plot(x[sort_inds], ygd[sort_inds], label='Gradient Descent')\n",
    "plt.plot(x[sort_inds], ysgd[sort_inds], label='Stochastic Gradient Descent')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
