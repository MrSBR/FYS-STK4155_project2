{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import functions as f\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#plt.style.available\n",
    "import seaborn as sns\n",
    "import load_data as ld\n",
    "import classes as cl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "import functions as f\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD:\n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000, lambda_reg=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.beta = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost_function(self, X, y):\n",
    "        N = len(y)\n",
    "        p = self.sigmoid(X @ self.beta)\n",
    "        cost = (-1 / N) * (y.T @ np.log(p) + (1 - y).T @ np.log(1 - p))\n",
    "        reg_term = (self.lambda_reg / (2 * N)) * np.sum(self.beta[1:] ** 2)\n",
    "        return cost + reg_term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, m = X.shape\n",
    "        self.beta = np.zeros(m)\n",
    "        cost_history = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            p = self.sigmoid(X @ self.beta)\n",
    "            gradient = (1 / N) * X.T @ (p - y)\n",
    "            # Apply regularization (exclude bias term)\n",
    "            gradient[1:] += (self.lambda_reg / N) * self.beta[1:]\n",
    "            self.beta -= self.learning_rate * gradient\n",
    "            cost = self.cost_function(X, y)\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        return cost_history\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        return self.sigmoid(X @ self.beta)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_prob(X) >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 11) (1000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((np\u001b[38;5;241m.\u001b[39mones((X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)), X))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m X, y \u001b[38;5;241m=\u001b[39m ld\u001b[38;5;241m.\u001b[39mload_simple_data()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Split the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mscale_train_test(X, y)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Sample data (replace with your dataset)\n",
    "# X: Features, y: Labels\n",
    "# For demonstration, let's create a synthetic dataset\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "# Add intercept term\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X, y = ld.load_simple_data()\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = f.scale_train_test(X, y)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 1]\n",
    "lambda_reg = 0.1\n",
    "n_iter = 1000\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LogisticRegressionGD(learning_rate=lr, n_iter=n_iter, lambda_reg=lambda_reg)\n",
    "    cost_history = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Learning Rate: {lr}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    plt.plot(cost_history, label=f'LR={lr}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Convergence for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lambda_values:\n\u001b[0;32m----> 5\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegressionGD(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, n_iter\u001b[38;5;241m=\u001b[39m\u001b[43mn_iter\u001b[49m, lambda_reg\u001b[38;5;241m=\u001b[39mlam)\n\u001b[1;32m      6\u001b[0m     cost_history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      7\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_iter' is not defined"
     ]
    }
   ],
   "source": [
    "lambda_values = [0.0, 0.01, 0.1, 1.0]\n",
    "learning_rate = 0.01\n",
    "\n",
    "for lam in lambda_values:\n",
    "    model = LogisticRegressionGD(learning_rate=learning_rate, n_iter=n_iter, lambda_reg=lam)\n",
    "    cost_history = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Lambda: {lam}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    plt.plot(cost_history, label=f'Lambda={lam}')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Convergence for Different Regularization Parameters')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         2        17.99         10.38          122.80     1001.0   \n",
      "1    842517         2        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         2        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         2        11.42         20.38           77.58      386.1   \n",
      "4  84358402         2        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data into a pandas DataFrame\n",
    "data = pd.read_csv(\"../../data/Wisconsin.csv\")\n",
    "\n",
    "# Change 'B' to 1 and 'M' to 2 in column 2\n",
    "data.loc[data['diagnosis'] == 'B', 'diagnosis'] = 1\n",
    "data.loc[data['diagnosis'] == 'M', 'diagnosis'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
